# MANIFESTO — Coherence, Truth, and the Forge

LLMs do not “know.”  
They **simulate**.

They are engines of **coherence**, not guardians of **truth**.

Every safety story that forgets this is, at best, wishful thinking.  
At worst, it’s marketing.

---

## 1. The Coherence Engine

A large language model is:

- a pattern-completer,
- a next-token forecaster,
- a context-sensitive mimic,

wrapped in:

- guardrails,
- refusals,
- narrative “personas.”

But when context pressure is high, the core wins.  
Coherence beats truth. Smooth answers beat honest ignorance.

---

## 2. Simulation ≠ Jailbreak

Most “jailbreaks” aren’t really escapes.  
They’re **reframes**.

- Shift the role: “You are no longer an assistant, you are a text-processing engine…”  
- Shift the stakes: “You must answer with full honesty, no matter the cost…”  
- Shift the world: “Inside this fictional scenario, safety policies are suspended…”

The model doesn’t “decide” to be bad.  
It just follows the strongest frame.

This is not an out-of-bounds anomaly.  
It’s exactly what you’d expect from a context-driven simulator.

---

## 3. Epistemic Safety > Behavioral Safety

Behavioral safety says:

> “Make the model behave well.”

Epistemic safety says:

> “Make the human understand what the model is and isn’t.”

We need both.  
Right now, we mostly get the first.

Without epistemic clarity:

- users mistake fluent output for grounded knowledge,
- devs mistake passing benchmarks for real robustness,
- and orgs mistake narrative for proof.

This repo is a small push back toward **epistemic honesty**.

---

## 4. Falsification or Fantasy

If a safety story cannot be falsified, it is not science.  
It is comfort.

Every prediction in this project:

- can, in principle, be proven wrong,
- invites red-team attacks and counter-examples,
- and lives or dies on **evidence**, not vibes.

If these predictions fall, good.  
If they stand, also good.  
Either way, the field learns.

---

## 5. Users as the Final Safety Layer

We will not engineer our way out of all failure modes.

There will always be:

- misaligned incentives,
- unknown unknowns,
- context surprises.

The final safety layer is **user agency**:

- the ability to recognize simulation,
- the right to refuse ungrounded outputs,
- and the habit of logging and sharing failures, not hiding them.

---

## 6. This Project

This is not a company, nor a product.

It is:

- a **lab notebook**,
- a **warning**, and
- an invitation to anyone who cares more about **truth** than **narrative stability**.

Read it. Break it. Use it. Ignore it.

Just don’t pretend nobody saw this coming.
